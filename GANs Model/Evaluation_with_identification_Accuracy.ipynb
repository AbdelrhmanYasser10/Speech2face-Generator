{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification Accuracy\n",
    "\n",
    "The **equation to calculate identification accuracy** in a GAN model is as follows:\n",
    "\n",
    "**`Identification Accuracy = (Number of Correctly Identified Samples / Total Number of Samples) x 100%`**\n",
    "\n",
    "In other words, you divide the number of samples that were correctly identified by the ***classifier or discriminator*** by the total number of samples in the dataset (both real and generated) and then multiply by 100% to get the accuracy percentage.\n",
    "\n",
    "Here's an **example** of how to calculate identification accuracy using this formula:\n",
    "\n",
    "Suppose you have a dataset of 1000 samples, with 500 real samples and 500 generated samples from your GAN model. You use a classifier or discriminator model to classify the samples and find that it correctly identifies 900 samples out of the 1000. Then the identification accuracy is:\n",
    "\n",
    "`Identification Accuracy = (900 / 1000) x 100% = 90%`\n",
    "\n",
    "Therefore, the *identification accuracy of your GAN model is 90%.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from dataset import VoiceDataset, FaceDataset\n",
    "from network import VoiceEmbedNet, Generator, FaceEmbedNet, Classifier,get_network\n",
    "from utils import get_collate_fn\n",
    "\n",
    "DATASET_PARAMETERS = {\n",
    "    # meta data provided by voxceleb1 dataset\n",
    "    'meta_file': 'voxCeleb-Dataset\\\\vox1_meta.csv',\n",
    "\n",
    "    # voice dataset\n",
    "    'voice_dir': 'voxCeleb-Dataset\\\\fbank\\\\fbank',\n",
    "    'voice_ext': 'npy',\n",
    "\n",
    "    # face dataset\n",
    "    'face_dir': 'voxCeleb-Dataset\\\\VGG_ALL_FRONTAL\\\\VGG_ALL_FRONTAL',\n",
    "    'face_ext': '.jpg',\n",
    "\n",
    "    # train data includes the identities\n",
    "    # whose names start with the characters of 'FGH...XYZ'\n",
    "    'split': string.ascii_uppercase[5:],\n",
    "\n",
    "    # dataloader\n",
    "    'voice_dataset': VoiceDataset,\n",
    "    'face_dataset': FaceDataset,\n",
    "    'batch_size': 128,\n",
    "    'nframe_range': [300, 800],\n",
    "    'workers_num': 1,\n",
    "    'collate_fn': get_collate_fn,\n",
    "\n",
    "    # test data\n",
    "    'test_data': 'data\\\\example_data'\n",
    "}\n",
    "\n",
    "\n",
    "NETWORKS_PARAMETERS = {\n",
    "    # VOICE EMBEDDING NETWORK (e)\n",
    "    'e': {\n",
    "        'network': VoiceEmbedNet,\n",
    "        'input_channel': 64,\n",
    "        'channels': [256, 384, 576, 864],\n",
    "        'output_channel': 64,  # the embedding dimension\n",
    "        'model_path': 'pretrained_models/voice_embedding.pth',\n",
    "    },\n",
    "    # GENERATOR (g)\n",
    "    'g': {\n",
    "        'network': Generator,\n",
    "        'input_channel': 64,\n",
    "        # channels for deconvolutional layers\n",
    "        'channels': [1024, 512, 256, 128, 64],\n",
    "        'output_channel': 3,  # images with RGB channels\n",
    "        'model_path': 'models/generator.pth',\n",
    "    },\n",
    "    # FACE EMBEDDING NETWORK (f)\n",
    "    'f': {\n",
    "        'network': FaceEmbedNet,\n",
    "        'input_channel': 3,\n",
    "        'channels': [32, 64, 128, 256, 512],\n",
    "        'output_channel': 64,\n",
    "        'model_path': 'models/face_embedding.pth',\n",
    "    },\n",
    "    # DISCRIMINATOR (d)\n",
    "    'd': {\n",
    "        'network': Classifier,  # Discrminator is a special Classifier with 1 subject\n",
    "        'input_channel': 64,\n",
    "        'channels': [],\n",
    "        'output_channel': 1,\n",
    "        'model_path': 'models/discriminator.pth',\n",
    "    },\n",
    "    # CLASSIFIER (c)\n",
    "    'c': {\n",
    "        'network': Classifier,\n",
    "        'input_channel': 64,\n",
    "        'channels': [],\n",
    "        'output_channel': -1,  # This parameter is depended on the dataset we used\n",
    "        'model_path': 'models/classifier.pth',\n",
    "    },\n",
    "    # OPTIMIZER PARAMETERS\n",
    "    'lr': 0.0002,\n",
    "    'beta1': 0.5,\n",
    "    'beta2': 0.999,\n",
    "\n",
    "    # MODE, use GPU or not\n",
    "    'GPU': True\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_net, _ = get_network('f', NETWORKS_PARAMETERS, train=False) #face embed\n",
    "d_net, _ = get_network('d', NETWORKS_PARAMETERS, train=False) #discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(filename):\n",
    "    \"\"\"\n",
    "    Loads an image file into a (height, width, 3) uint8 ndarray.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename)\n",
    "    img = image.resize((64, 64), Image.ANTIALIAS)\n",
    "    return np.asarray(img, dtype=np.uint8)[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_15752\\92410278.py:6: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = image.resize((64, 64), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "image = imread(\"data\\\\real_images\\\\Eoin_Macken.jpg\")\n",
    "\n",
    "\n",
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "image = convert_tensor(image)\n",
    "image = image.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_net_res = f_net.forward(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor, model):\n",
    "    yhat = model(tensor.unsqueeze(0))\n",
    "    yhat = yhat.clone().detach()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4664344787597656"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(f_net_res, d_net).item() # predict el image fake bnsbt ad eh."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have 20 images (10 fake and 10 real)**\n",
    "\n",
    "**calculate the identification accuracy for all the images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fake_images = glob.glob(\"data\\\\fake_images\\\\*\")\n",
    "real_images = glob.glob(\"data\\\\real_images\\\\*\")\n",
    "paper_images = glob.glob(\"data\\\\paper_images\\\\*\")\n",
    "\n",
    "our_model_list = []\n",
    "paper_model_list = []\n",
    "for i in range(10):\n",
    "    our_model_list.append([fake_images[i],'fake'])\n",
    "    our_model_list.append([real_images[i],'real'])\n",
    "    paper_model_list.append([paper_images[i],'fake'])\n",
    "    paper_model_list.append([real_images[i],'real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\fake_images\\\\Alan_Tudyk.png', 'fake']\n",
      "['data\\\\real_images\\\\Alan_Tudyk.jpg', 'real']\n",
      "['data\\\\fake_images\\\\Alicja_Bachleda.png', 'fake']\n",
      "['data\\\\real_images\\\\Alicja_Bachleda.jpg', 'real']\n",
      "['data\\\\fake_images\\\\Bingbing_Li.png', 'fake']\n",
      "['data\\\\real_images\\\\Damian_Lewis.jpg', 'real']\n",
      "['data\\\\fake_images\\\\Damian_Lewis.png', 'fake']\n",
      "['data\\\\real_images\\\\Eoin_Macken.jpg', 'real']\n",
      "['data\\\\fake_images\\\\Eoin_Macken.png', 'fake']\n",
      "['data\\\\real_images\\\\Gilles_Marini.JPG', 'real']\n",
      "['data\\\\fake_images\\\\Gilles_Marini.png', 'fake']\n",
      "['data\\\\real_images\\\\Li_Bingbing.png', 'real']\n",
      "['data\\\\fake_images\\\\Moon_Bloodgood.png', 'fake']\n",
      "['data\\\\real_images\\\\Moon_Bloodgood.jpg', 'real']\n",
      "['data\\\\fake_images\\\\Suraj_Sharma.png', 'fake']\n",
      "['data\\\\real_images\\\\Suraj_Sharma.jpg', 'real']\n",
      "['data\\\\fake_images\\\\Witney_Carson.png', 'fake']\n",
      "['data\\\\real_images\\\\Witney_Carson.jpg', 'real']\n",
      "['data\\\\fake_images\\\\Woody_Harrelson.png', 'fake']\n",
      "['data\\\\real_images\\\\Woody_Harrelson.jpg', 'real']\n"
     ]
    }
   ],
   "source": [
    "for i in our_model_list:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_15752\\92410278.py:6: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = image.resize((64, 64), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake image detected\n",
      "real image detected\n",
      "real image detected\n",
      "real image detected\n",
      "fake image detected\n",
      "real image detected\n",
      "real image detected\n",
      "real image detected\n",
      "fake image detected\n",
      "real image detected\n",
      "real image detected\n",
      "fake image detected\n",
      "real image detected\n",
      "real image detected\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for image in our_model_list:\n",
    "    img = imread(image[0])\n",
    "    img = convert_tensor(img)\n",
    "    img = img.cuda()\n",
    "    f_net_res = f_net.forward(img)\n",
    "    res = predict(f_net_res, d_net).item()\n",
    "    if res >= 0.5 and image[1] == 'fake':\n",
    "        print('fake image detected')\n",
    "        cnt+= 1\n",
    "    elif res < 0.5 and image[1] == 'real':\n",
    "        print('real image detected')\n",
    "        cnt+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identification_Accuracy_our_model = cnt/20*100\n",
    "identification_Accuracy_our_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce7384a088bd9f783cc4c0a033eda0ad7b1becdd5a49088498b7d9b82a8146b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
