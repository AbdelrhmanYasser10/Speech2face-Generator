{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from dataset import VoiceDataset, FaceDataset\n",
    "from network import VoiceEmbedNet, Generator, FaceEmbedNet, Classifier,get_network\n",
    "from utils import get_collate_fn\n",
    "\n",
    "DATASET_PARAMETERS = {\n",
    "    # meta data provided by voxceleb1 dataset\n",
    "    'meta_file': 'voxCeleb-Dataset\\\\vox1_meta.csv',\n",
    "\n",
    "    # voice dataset\n",
    "    'voice_dir': 'voxCeleb-Dataset\\\\fbank\\\\fbank',\n",
    "    'voice_ext': 'npy',\n",
    "\n",
    "    # face dataset\n",
    "    'face_dir': 'voxCeleb-Dataset\\\\VGG_ALL_FRONTAL\\\\VGG_ALL_FRONTAL',\n",
    "    'face_ext': '.jpg',\n",
    "\n",
    "    # train data includes the identities\n",
    "    # whose names start with the characters of 'FGH...XYZ'\n",
    "    'split': string.ascii_uppercase[5:],\n",
    "\n",
    "    # dataloader\n",
    "    'voice_dataset': VoiceDataset,\n",
    "    'face_dataset': FaceDataset,\n",
    "    'batch_size': 128,\n",
    "    'nframe_range': [300, 800],\n",
    "    'workers_num': 1,\n",
    "    'collate_fn': get_collate_fn,\n",
    "\n",
    "    # test data\n",
    "    'test_data': 'data\\\\example_data'\n",
    "}\n",
    "\n",
    "\n",
    "NETWORKS_PARAMETERS = {\n",
    "    # VOICE EMBEDDING NETWORK (e)\n",
    "    'e': {\n",
    "        'network': VoiceEmbedNet,\n",
    "        'input_channel': 64,\n",
    "        'channels': [256, 384, 576, 864],\n",
    "        'output_channel': 64,  # the embedding dimension\n",
    "        'model_path': 'pretrained_models/voice_embedding.pth',\n",
    "    },\n",
    "    # GENERATOR (g)\n",
    "    'g': {\n",
    "        'network': Generator,\n",
    "        'input_channel': 64,\n",
    "        # channels for deconvolutional layers\n",
    "        'channels': [1024, 512, 256, 128, 64],\n",
    "        'output_channel': 3,  # images with RGB channels\n",
    "        'model_path': 'models/generator.pth',\n",
    "    },\n",
    "    # FACE EMBEDDING NETWORK (f)\n",
    "    'f': {\n",
    "        'network': FaceEmbedNet,\n",
    "        'input_channel': 3,\n",
    "        'channels': [32, 64, 128, 256, 512],\n",
    "        'output_channel': 64,\n",
    "        'model_path': 'models/face_embedding.pth',\n",
    "    },\n",
    "    # DISCRIMINATOR (d)\n",
    "    'd': {\n",
    "        'network': Classifier,  # Discrminator is a special Classifier with 1 subject\n",
    "        'input_channel': 64,\n",
    "        'channels': [],\n",
    "        'output_channel': 1,\n",
    "        'model_path': 'models/discriminator.pth',\n",
    "    },\n",
    "    # CLASSIFIER (c)\n",
    "    'c': {\n",
    "        'network': Classifier,\n",
    "        'input_channel': 64,\n",
    "        'channels': [],\n",
    "        'output_channel': -1,  # This parameter is depended on the dataset we used\n",
    "        'model_path': 'models/classifier.pth',\n",
    "    },\n",
    "    # OPTIMIZER PARAMETERS\n",
    "    'lr': 0.0002,\n",
    "    'beta1': 0.5,\n",
    "    'beta2': 0.999,\n",
    "\n",
    "    # MODE, use GPU or not\n",
    "    'GPU': True\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_net, _ = get_network('f', NETWORKS_PARAMETERS, train=False) #face embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "def imread(filename):\n",
    "    \"\"\"\n",
    "    Loads an image file into a (height, width, 3) uint8 ndarray.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename)\n",
    "    img = image.resize((64, 64), Image.ANTIALIAS)\n",
    "    return np.asarray(img, dtype=np.uint8)[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fake_images = glob.glob(\"data\\\\fake_images\\\\*\")\n",
    "real_images = glob.glob(\"data\\\\real_images\\\\*\")\n",
    "paper_images = glob.glob(\"data\\\\paper_images\\\\*\")\n",
    "\n",
    "our_model_list = []\n",
    "paper_model_list = []\n",
    "for i in range(10):\n",
    "    our_model_list.append([fake_images[i],real_images[i]])\n",
    "    paper_model_list.append([paper_images[i],real_images[i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_10120\\1875066787.py:11: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = image.resize((64, 64), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9985]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9527]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9760]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9735]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9949]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9952]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9967]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9069]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9818]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([[0.9849]], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cos_sim_list_ours = []\n",
    "convert_tensor = transforms.ToTensor()\n",
    "for image in our_model_list:\n",
    "    #print(image)\n",
    "    imgf = imread(image[0])\n",
    "    imgf = convert_tensor(imgf)\n",
    "    imgf = imgf.cuda()\n",
    "    f_net_res_f = f_net.forward(imgf)\n",
    "    \n",
    "    imgr = imread(image[1])\n",
    "    imgr = convert_tensor(imgr)\n",
    "    imgr = imgr.cuda()\n",
    "    f_net_res_r = f_net.forward(imgr)\n",
    "    \n",
    "    cosine_similarity_value = F.cosine_similarity(f_net_res_f, f_net_res_r, dim=0)\n",
    "    cos_sim_list_ours.append(cosine_similarity_value.item())\n",
    "    print(cosine_similarity_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9761002659797668"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_score_ours = sum(cos_sim_list_ours)/len(cos_sim_list_ours)\n",
    "cos_sim_score_ours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_10120\\1875066787.py:11: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = image.resize((64, 64), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1645336151123047]\n",
      "[0.1645336151123047, 0.9962781667709351]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005, 0.5801464915275574]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005, 0.5801464915275574, 0.25248992443084717]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005, 0.5801464915275574, 0.25248992443084717, 0.3938966989517212]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005, 0.5801464915275574, 0.25248992443084717, 0.3938966989517212, 0.4451752007007599]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005, 0.5801464915275574, 0.25248992443084717, 0.3938966989517212, 0.4451752007007599, 1.1131722927093506]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005, 0.5801464915275574, 0.25248992443084717, 0.3938966989517212, 0.4451752007007599, 1.1131722927093506, 0.457733690738678]\n",
      "[0.1645336151123047, 0.9962781667709351, 0.6004239320755005, 0.5801464915275574, 0.25248992443084717, 0.3938966989517212, 0.4451752007007599, 1.1131722927093506, 0.457733690738678, 0.6288408041000366]\n"
     ]
    }
   ],
   "source": [
    "l1 = []\n",
    "convert_tensor = transforms.ToTensor()\n",
    "for image in our_model_list:\n",
    "    #print(image)\n",
    "    imgf = imread(image[0])\n",
    "    imgf = convert_tensor(imgf)\n",
    "    imgf = imgf.cuda()\n",
    "    f_net_res_f = f_net.forward(imgf)\n",
    "    \n",
    "    imgr = imread(image[1])\n",
    "    imgr = convert_tensor(imgr)\n",
    "    imgr = imgr.cuda()\n",
    "    f_net_res_r = f_net.forward(imgr)\n",
    "    \n",
    "    loss = nn.L1Loss()\n",
    "    output = loss(f_net_res_f, f_net_res_r)\n",
    "    l1.append(output.item())\n",
    "    print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5632690817117691"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_avg = sum(l1)/len(l1)\n",
    "l1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
