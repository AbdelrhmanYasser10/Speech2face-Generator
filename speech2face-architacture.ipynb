{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cc9d9a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:30.099824Z",
     "iopub.status.busy": "2023-02-13T20:16:30.098954Z",
     "iopub.status.idle": "2023-02-13T20:16:32.283639Z",
     "shell.execute_reply": "2023-02-13T20:16:32.282243Z"
    },
    "papermill": {
     "duration": 2.196505,
     "end_time": "2023-02-13T20:16:32.286929",
     "exception": false,
     "start_time": "2023-02-13T20:16:30.090424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8769ce21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.297786Z",
     "iopub.status.busy": "2023-02-13T20:16:32.296883Z",
     "iopub.status.idle": "2023-02-13T20:16:32.311650Z",
     "shell.execute_reply": "2023-02-13T20:16:32.309301Z"
    },
    "papermill": {
     "duration": 0.023522,
     "end_time": "2023-02-13T20:16:32.314757",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.291235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VoiceEmbedNet(nn.Module):\n",
    "    def __init__(self, input_channel, channels, output_channel):\n",
    "        super(VoiceEmbedNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(input_channel, channels[0], 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(channels[0], affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels[0], channels[1], 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(channels[1], affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels[1], channels[2], 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(channels[2], affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels[2], channels[3], 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(channels[3], affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels[3], output_channel, 3, 2, 1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = nn.functional.avg_pool1d(x, x.size()[2], stride=1)\n",
    "        x = x.view(x.size()[0], -1, 1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d28e484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.325738Z",
     "iopub.status.busy": "2023-02-13T20:16:32.325208Z",
     "iopub.status.idle": "2023-02-13T20:16:32.338172Z",
     "shell.execute_reply": "2023-02-13T20:16:32.336382Z"
    },
    "papermill": {
     "duration": 0.022144,
     "end_time": "2023-02-13T20:16:32.341295",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.319151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channel, channels, output_channel):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(input_channel, channels[0], 4, 1, 0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(channels[0], channels[1], 4, 2, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(channels[1], channels[2], 4, 2, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(channels[2], channels[3], 4, 2, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(channels[3], channels[4], 4, 2, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(channels[4], output_channel, 1, 1, 0, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74072d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.352881Z",
     "iopub.status.busy": "2023-02-13T20:16:32.352416Z",
     "iopub.status.idle": "2023-02-13T20:16:32.364276Z",
     "shell.execute_reply": "2023-02-13T20:16:32.362931Z"
    },
    "papermill": {
     "duration": 0.021132,
     "end_time": "2023-02-13T20:16:32.367121",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.345989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceEmbedNet(nn.Module):\n",
    "    def __init__(self, input_channel, channels, output_channel):\n",
    "        super(FaceEmbedNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, channels[0], 1, 1, 0, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels[0], channels[1], 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels[1], channels[2], 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels[2], channels[3], 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels[3], channels[4], 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels[4], output_channel, 4, 1, 0, bias=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee932ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.377764Z",
     "iopub.status.busy": "2023-02-13T20:16:32.377224Z",
     "iopub.status.idle": "2023-02-13T20:16:32.384978Z",
     "shell.execute_reply": "2023-02-13T20:16:32.383679Z"
    },
    "papermill": {
     "duration": 0.016332,
     "end_time": "2023-02-13T20:16:32.387624",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.371292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_channel, channels, output_channel):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = nn.Linear(input_channel, output_channel, bias=False)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d12ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.397540Z",
     "iopub.status.busy": "2023-02-13T20:16:32.397003Z",
     "iopub.status.idle": "2023-02-13T20:16:32.406002Z",
     "shell.execute_reply": "2023-02-13T20:16:32.404185Z"
    },
    "papermill": {
     "duration": 0.017286,
     "end_time": "2023-02-13T20:16:32.408935",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.391649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_network(net_type, params, train=True):\n",
    "    net_params = params[net_type]\n",
    "    net = net_params['network'](net_params['input_channel'],\n",
    "                                net_params['channels'],\n",
    "                                net_params['output_channel'])\n",
    "    if train:\n",
    "        net.train()\n",
    "        optimizer = optim.Adam(net.parameters(),lr=params['lr'],betas=(params['beta1'], params['beta2']))\n",
    "    else:\n",
    "        net.eval()\n",
    "        optimizer = None\n",
    "    return net, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8a9e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.420063Z",
     "iopub.status.busy": "2023-02-13T20:16:32.419216Z",
     "iopub.status.idle": "2023-02-13T20:16:32.429103Z",
     "shell.execute_reply": "2023-02-13T20:16:32.427559Z"
    },
    "papermill": {
     "duration": 0.018734,
     "end_time": "2023-02-13T20:16:32.432039",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.413305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NETWORKS_PARAMETERS = {\n",
    "    # VOICE EMBEDDING NETWORK (e)\n",
    "    'e': {\n",
    "        'network': VoiceEmbedNet,\n",
    "        'input_channel': 64,\n",
    "        'channels': [256, 384, 576, 864],\n",
    "        'output_channel': 64, # the embedding dimension\n",
    "    },\n",
    "    # GENERATOR (g)\n",
    "    'g': {\n",
    "        'network': Generator,\n",
    "        'input_channel': 64,\n",
    "        'channels': [1024, 512, 256, 128, 64], # channels for deconvolutional layers\n",
    "        'output_channel': 3, # images with RGB channels\n",
    "    },\n",
    "    # FACE EMBEDDING NETWORK (f)\n",
    "    'f': {\n",
    "        'network': FaceEmbedNet,\n",
    "        'input_channel': 3,\n",
    "        'channels': [32, 64, 128, 256, 512],\n",
    "        'output_channel': 64,\n",
    "    },\n",
    "    # DISCRIMINATOR (d)\n",
    "    'd': {\n",
    "        'network': Classifier, # Discrminator is a special Classifier with 1 subject\n",
    "        'input_channel': 64,\n",
    "        'channels': [],\n",
    "        'output_channel': 1,\n",
    "    },\n",
    "    # CLASSIFIER (c)\n",
    "    'c': {\n",
    "        'network': Classifier,\n",
    "        'input_channel': 64,\n",
    "        'channels': [],\n",
    "        'output_channel': 2, # This parameter is depended on the dataset we used\n",
    "    },\n",
    "    # OPTIMIZER PARAMETERS \n",
    "    'lr': 0.0002,\n",
    "    'beta1': 0.5,\n",
    "    'beta2': 0.999\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58d96556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.443393Z",
     "iopub.status.busy": "2023-02-13T20:16:32.442654Z",
     "iopub.status.idle": "2023-02-13T20:16:32.672176Z",
     "shell.execute_reply": "2023-02-13T20:16:32.670396Z"
    },
    "papermill": {
     "duration": 0.239331,
     "end_time": "2023-02-13T20:16:32.675902",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.436571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "e_net, e_optimizer = get_network('e', NETWORKS_PARAMETERS, train=False)\n",
    "g_net, g_optimizer = get_network('g', NETWORKS_PARAMETERS, train=True)\n",
    "f_net, f_optimizer = get_network('f', NETWORKS_PARAMETERS, train=True)\n",
    "d_net, d_optimizer = get_network('d', NETWORKS_PARAMETERS, train=True)\n",
    "c_net, c_optimizer = get_network('c', NETWORKS_PARAMETERS, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71676552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.687160Z",
     "iopub.status.busy": "2023-02-13T20:16:32.686290Z",
     "iopub.status.idle": "2023-02-13T20:16:32.691100Z",
     "shell.execute_reply": "2023-02-13T20:16:32.689984Z"
    },
    "papermill": {
     "duration": 0.01321,
     "end_time": "2023-02-13T20:16:32.693515",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.680305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialise first label for real/fake faces (keep the same names)\n",
    "#real_label ,fake_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1c1ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T20:16:32.703688Z",
     "iopub.status.busy": "2023-02-13T20:16:32.703231Z",
     "iopub.status.idle": "2023-02-13T20:16:32.716487Z",
     "shell.execute_reply": "2023-02-13T20:16:32.715208Z"
    },
    "papermill": {
     "duration": 0.021898,
     "end_time": "2023-02-13T20:16:32.719622",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.697724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(10000):\\n    # You have to fill first those 4 parameters from your dataset (keep the same names)\\n    #voice, voice_label , face, face_label\\n    noise = 0.05*torch.randn(128, 64, 1, 1)\\n\\n    embeddings = e_net(voice)\\n    embeddings = nn.functional.normalize(embeddings)\\n    embeddings = embeddings + noise\\n    embeddings = nn.functional.normalize(embeddings)\\n    fake = g_net(embeddings)\\n\\n    # Discriminator\\n    f_optimizer.zero_grad()\\n    d_optimizer.zero_grad()\\n    c_optimizer.zero_grad()\\n    real_score_out = d_net(f_net(face))\\n    fake_score_out = d_net(f_net(fake.detach()))\\n    real_label_out = c_net(f_net(face))\\n    D_real_loss = nn.functional.binary_cross_entropy(torch.sigmoid(real_score_out), real_label)\\n    D_fake_loss = nn.functional.binary_cross_entropy(torch.sigmoid(fake_score_out), fake_label)\\n    C_real_loss = nn.functional.nll_loss(nn.functional.log_softmax(real_label_out, 1), face_label)\\n    D_real.update(D_real_loss.item())\\n    D_fake.update(D_fake_loss.item())\\n    C_real.update(C_real_loss.item())\\n    (D_real_loss + D_fake_loss + C_real_loss).backward()\\n    f_optimizer.step()\\n    d_optimizer.step()\\n    c_optimizer.step()\\n\\n    # Generator\\n    g_optimizer.zero_grad()\\n    fake_score_out = d_net(f_net(fake))\\n    fake_label_out = c_net(f_net(fake))\\n    GD_fake_loss = nn.functional.binary_cross_entropy(torch.sigmoid(fake_score_out), real_label)\\n    GC_fake_loss = nn.functional.nll_loss(nn.functional.log_softmax(fake_label_out, 1), voice_label)\\n    (GD_fake_loss + GC_fake_loss).backward()\\n    GD_fake.update(GD_fake_loss.item())\\n    GC_fake.update(GC_fake_loss.item())\\n    g_optimizer.step()\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(10000):\n",
    "    # You have to fill first those 4 parameters from your dataset (keep the same names)\n",
    "    #voice, voice_label , face, face_label\n",
    "    noise = 0.05*torch.randn(128, 64, 1, 1)\n",
    "\n",
    "    embeddings = e_net(voice)\n",
    "    embeddings = nn.functional.normalize(embeddings)\n",
    "    embeddings = embeddings + noise\n",
    "    embeddings = nn.functional.normalize(embeddings)\n",
    "    fake = g_net(embeddings)\n",
    "\n",
    "    # Discriminator\n",
    "    f_optimizer.zero_grad()\n",
    "    d_optimizer.zero_grad()\n",
    "    c_optimizer.zero_grad()\n",
    "    real_score_out = d_net(f_net(face))\n",
    "    fake_score_out = d_net(f_net(fake.detach()))\n",
    "    real_label_out = c_net(f_net(face))\n",
    "    D_real_loss = nn.functional.binary_cross_entropy(torch.sigmoid(real_score_out), real_label)\n",
    "    D_fake_loss = nn.functional.binary_cross_entropy(torch.sigmoid(fake_score_out), fake_label)\n",
    "    C_real_loss = nn.functional.nll_loss(nn.functional.log_softmax(real_label_out, 1), face_label)\n",
    "    D_real.update(D_real_loss.item())\n",
    "    D_fake.update(D_fake_loss.item())\n",
    "    C_real.update(C_real_loss.item())\n",
    "    (D_real_loss + D_fake_loss + C_real_loss).backward()\n",
    "    f_optimizer.step()\n",
    "    d_optimizer.step()\n",
    "    c_optimizer.step()\n",
    "\n",
    "    # Generator\n",
    "    g_optimizer.zero_grad()\n",
    "    fake_score_out = d_net(f_net(fake))\n",
    "    fake_label_out = c_net(f_net(fake))\n",
    "    GD_fake_loss = nn.functional.binary_cross_entropy(torch.sigmoid(fake_score_out), real_label)\n",
    "    GC_fake_loss = nn.functional.nll_loss(nn.functional.log_softmax(fake_label_out, 1), voice_label)\n",
    "    (GD_fake_loss + GC_fake_loss).backward()\n",
    "    GD_fake.update(GD_fake_loss.item())\n",
    "    GC_fake.update(GC_fake_loss.item())\n",
    "    g_optimizer.step()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77400d20",
   "metadata": {
    "papermill": {
     "duration": 0.00395,
     "end_time": "2023-02-13T20:16:32.728048",
     "exception": false,
     "start_time": "2023-02-13T20:16:32.724098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.303107,
   "end_time": "2023-02-13T20:16:33.962325",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-13T20:16:18.659218",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
