{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from config import DATASET_PARAMETERS, NETWORKS_PARAMETERS\n",
    "from network import get_network\n",
    "from preprocessing_train import get_dataset, Meter, cycle, save_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "print('Parsing your dataset...')\n",
    "voice_list, face_list, id_class_num = get_dataset(DATASET_PARAMETERS)\n",
    "NETWORKS_PARAMETERS['c']['output_channel'] = id_class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing the datasets...')\n",
    "voice_dataset = DATASET_PARAMETERS['voice_dataset'](voice_list,\n",
    "                               DATASET_PARAMETERS['nframe_range'])\n",
    "face_dataset = DATASET_PARAMETERS['face_dataset'](face_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing the dataloaders...')\n",
    "collate_fn = DATASET_PARAMETERS['collate_fn'](DATASET_PARAMETERS['nframe_range'])\n",
    "voice_loader = DataLoader(voice_dataset, shuffle=True, drop_last=True,\n",
    "                          batch_size=DATASET_PARAMETERS['batch_size'],\n",
    "                          num_workers=DATASET_PARAMETERS['workers_num'],\n",
    "                          collate_fn=collate_fn)\n",
    "face_loader = DataLoader(face_dataset, shuffle=True, drop_last=True,\n",
    "                         batch_size=DATASET_PARAMETERS['batch_size'],\n",
    "                         num_workers=DATASET_PARAMETERS['workers_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_iterator = iter(cycle(voice_loader))\n",
    "face_iterator = iter(cycle(face_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# networks, Fe, Fg, Fd (f+d), Fc (f+c)\n",
    "print('Initializing networks...')\n",
    "e_net, e_optimizer = get_network('e', NETWORKS_PARAMETERS, train=False)\n",
    "g_net, g_optimizer = get_network('g', NETWORKS_PARAMETERS, train=True)\n",
    "f_net, f_optimizer = get_network('f', NETWORKS_PARAMETERS, train=True)\n",
    "d_net, d_optimizer = get_network('d', NETWORKS_PARAMETERS, train=True)\n",
    "c_net, c_optimizer = get_network('c', NETWORKS_PARAMETERS, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label for real/fake faces\n",
    "real_label = torch.full((DATASET_PARAMETERS['batch_size'], 1), 1)\n",
    "fake_label = torch.full((DATASET_PARAMETERS['batch_size'], 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meters for recording the training status\n",
    "iteration = Meter('Iter', 'sum', ':5d')\n",
    "data_time = Meter('Data', 'sum', ':4.2f')\n",
    "batch_time = Meter('Time', 'sum', ':4.2f')\n",
    "D_real = Meter('D_real', 'avg', ':3.2f')\n",
    "D_fake = Meter('D_fake', 'avg', ':3.2f')\n",
    "C_real = Meter('C_real', 'avg', ':3.2f')\n",
    "GD_fake = Meter('G_D_fake', 'avg', ':3.2f')\n",
    "GC_fake = Meter('G_C_fake', 'avg', ':3.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training models...')\n",
    "for it in range(1000):\n",
    "    # data\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #x_axis.append(it)\n",
    "    \n",
    "    voice, voice_label = next(voice_iterator)\n",
    "    face, face_label = next(face_iterator)\n",
    "    noise = 0.05*torch.randn(DATASET_PARAMETERS['batch_size'], 64, 1, 1)\n",
    "\n",
    "    # use GPU or not\n",
    "    \n",
    "    if NETWORKS_PARAMETERS['GPU']: \n",
    "        voice, voice_label = voice.cuda(), voice_label.cuda()\n",
    "        face, face_label = face.cuda(), face_label.cuda()\n",
    "        real_label, fake_label = real_label.cuda(), fake_label.cuda()\n",
    "        noise = noise.cuda()\n",
    "    data_time.update(time.time() - start_time)\n",
    "    \n",
    "    # get embeddings and generated faces\n",
    "    embeddings = e_net(voice)\n",
    "    embeddings = F.normalize(embeddings)\n",
    "    # introduce some permutations\n",
    "    embeddings = embeddings + noise\n",
    "    embeddings = F.normalize(embeddings)\n",
    "    fake = g_net(embeddings)\n",
    "\n",
    "    # Discriminator\n",
    "    f_optimizer.zero_grad()\n",
    "    d_optimizer.zero_grad()\n",
    "    c_optimizer.zero_grad()\n",
    "    \n",
    "    real_score_out = d_net(f_net(face))\n",
    "    fake_score_out = d_net(f_net(fake.detach()))\n",
    "    real_label_out = c_net(f_net(face))\n",
    "    \n",
    "    D_real_loss = F.binary_cross_entropy(torch.sigmoid(real_score_out), real_label.type(torch.cuda.FloatTensor))\n",
    "    D_fake_loss = F.binary_cross_entropy(torch.sigmoid(fake_score_out), fake_label.type(torch.cuda.FloatTensor))\n",
    "    C_real_loss = F.nll_loss(F.log_softmax(real_label_out, 1), face_label)\n",
    "    \n",
    "    D_real.update(D_real_loss.item())\n",
    "    D_fake.update(D_fake_loss.item())\n",
    "    C_real.update(C_real_loss.item())\n",
    "    \n",
    "    #discriminator_losses.append((D_real_loss + D_fake_loss + C_real_loss).item())\n",
    "    \n",
    "    (D_real_loss + D_fake_loss + C_real_loss).backward()\n",
    "    f_optimizer.step()\n",
    "    d_optimizer.step()\n",
    "    c_optimizer.step()\n",
    "\n",
    "    # Generator\n",
    "    g_optimizer.zero_grad()\n",
    "    fake_score_out = d_net(f_net(fake))\n",
    "    fake_label_out = c_net(f_net(fake))\n",
    "    GD_fake_loss = F.binary_cross_entropy(torch.sigmoid(fake_score_out), real_label.type(torch.cuda.FloatTensor))\n",
    "    GC_fake_loss = F.nll_loss(F.log_softmax(fake_label_out, 1), voice_label)\n",
    "    \n",
    "    #generator_losses.append((GD_fake_loss + GC_fake_loss).item())\n",
    "    \n",
    "    (GD_fake_loss + GC_fake_loss).backward()\n",
    "    GD_fake.update(GD_fake_loss.item())\n",
    "    GC_fake.update(GC_fake_loss.item())\n",
    "    g_optimizer.step()\n",
    "\n",
    "    #times.append(time.time() - start_time)\n",
    "    \n",
    "    batch_time.update(time.time() - start_time)\n",
    "    #df = pd.DataFrame({'Epoch': x_axis,'Generator loss': generator_losses, 'Discriminator loss': discriminator_losses})\n",
    "    # print status\n",
    "    if it % 200 == 0:\n",
    "        print(iteration, data_time, batch_time,D_real, D_fake, C_real, GD_fake, GC_fake)\n",
    "        data_time.reset()\n",
    "        batch_time.reset()\n",
    "        D_real.reset()\n",
    "        D_fake.reset()\n",
    "        C_real.reset()\n",
    "        GD_fake.reset()\n",
    "        GC_fake.reset()\n",
    "        # snapshot\n",
    "        save_model(g_net, NETWORKS_PARAMETERS['g']['model_path'])\n",
    "        save_model(f_net, NETWORKS_PARAMETERS['f']['model_path'])\n",
    "        save_model(d_net, NETWORKS_PARAMETERS['d']['model_path'])\n",
    "        save_model(c_net, NETWORKS_PARAMETERS['c']['model_path'])\n",
    "        #df.to_csv('loss.csv', index=False, header=True)\n",
    "    iteration.update(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
